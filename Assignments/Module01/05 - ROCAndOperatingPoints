Open SupportCode\Framework-5-ROCAndOperatingPoints.py

Run the framework and look at the ROC curve that it outputs. Make sure to check out the
  TabulateModelPerformanceForROC helper function.

HAND IN:

Note - for this assignment you can quantize the thresholds you try to the nearest percent, that is try thresholds [0.0, 0.01, 0.02 ... 0.99, 1.0]
 and use the closest value you find with that method (which is what the TabulateModelPerformanceForROC helper function will do for you).

1.0 point - Answer the following questions about the models produced by the framework code:
    Which model is better at a 50% False positive rate?
    Which model is better at a 10% False positive rate?
    Which model is better at a 40% False negative rate?
    What classification threshold would you use with the numFrequentWords = 25 model to achieve a 10% False positive rate?

Tune the hyperperamters 'numMutualInformationWords' and 'numFrequentWords' until you find a new model
that is better than both of the models in the framework code at both the 50% and the 10% false positive rate.

1.0 points - In a few sentences, describe the strategy you used to find the better hyperparameters and 
  the hyperperparameters you found to achieve the result? What were the hyperparameters?

1.0 points - Create an ROC plot showing:
   * the model with 25 features by frequency;
   * the model with 25 features by mutual inforamtion;
   * and the model you produced by tuning the hyperparamters in the previous question.
   
   Make sure to label everything clearly!
